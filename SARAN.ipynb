{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc5ffb9",
   "metadata": {},
   "source": [
    "# SARAN: Shallow Auto-Regressive Attention Network\n",
    "\n",
    "The dominant paradigms in sequence transduction - Recurrent Neural Networks and deep Transformer architectures - rely on complex, multi-layered structures to achieve performance, often at the cost of interpretability and computational transparency. In this work, we introduce the Shallow Auto-Regressive Attention Network (SARAN), a minimalist architecture that reduces the Transformer decoder to its fundamental components. SARAN is defined by a strictly linear, 15-stage computational graph that maps input embeddings directly to output probabilities via a single, isolated block of masked self-attention. We present a \"first principles\" derivation of the network's training dynamics, explicitly defining the manual backpropagation algorithm through the attention mechanism without reliance on automatic differentiation engines. By stripping away deep layer stacking and feed-forward networks, SARAN demonstrates that a solitary attention block is sufficient to mechanically derive autoregressive properties, providing a transparent and rigorous baseline for understanding the mechanics of attention-based sequence modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe6e1b",
   "metadata": {},
   "source": [
    "### [dataset](https://www.kaggle.com/datasets/mytechnotalent/mary-had-a-little-lamb)\n",
    "\n",
    "Author: [Kevin Thomas](mailto:ket189@pitt.edu)\n",
    "\n",
    "License: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9739d9c",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffa09c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = 3.141592653589793\n",
    "RNG_STATE = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391a954",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eb2fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_ids(tokens, word_to_idx):\n",
    "    unk = word_to_idx.get('<UNK>')\n",
    "    return [word_to_idx.get(w, unk) for w in tokens]\n",
    "\n",
    "\n",
    "def random():\n",
    "    global RNG_STATE\n",
    "    RNG_STATE = (1103515245 * RNG_STATE + 12345) % (2**31)\n",
    "    return RNG_STATE / (2**31)\n",
    "\n",
    "\n",
    "def sqrt(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    guess = x / 2.0\n",
    "    for _ in range(20):\n",
    "        guess = (guess + x / guess) / 2.0\n",
    "    return guess\n",
    "\n",
    "\n",
    "def exp(x):\n",
    "    if x < -10:\n",
    "        return 1.0 / exp(-x)\n",
    "    result = 1.0\n",
    "    term = 1.0\n",
    "    for i in range(1, 50):\n",
    "        term *= x / i\n",
    "        result += term\n",
    "        if abs(term) < 1e-10:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "\n",
    "def log(x):\n",
    "    if x <= 0:\n",
    "        return float('-inf')\n",
    "    guess = 0.0\n",
    "    for _ in range(50):\n",
    "        guess = guess + 2 * (x - exp(guess)) / (x + exp(guess))\n",
    "    return guess\n",
    "\n",
    "\n",
    "def cos(x):\n",
    "    x = x % (2 * PI)\n",
    "    result = 1.0\n",
    "    term = 1.0\n",
    "    for i in range(1, 20):\n",
    "        term *= -x * x / ((2 * i - 1) * (2 * i))\n",
    "        result += term\n",
    "    return result\n",
    "\n",
    "\n",
    "def randn():\n",
    "    u1 = random()\n",
    "    u2 = random()\n",
    "    return sqrt(-2.0 * log(u1)) * cos(2.0 * PI * u2)\n",
    "\n",
    "\n",
    "def parse_json_array(text):\n",
    "    text = text.strip()\n",
    "    if not text.startswith('[') or not text.endswith(']'):\n",
    "        return []\n",
    "    text = text[1:-1]  # Remove [ ]\n",
    "    result = []\n",
    "    in_string = False\n",
    "    current = \"\"\n",
    "    escape = False\n",
    "    for char in text:\n",
    "        if escape:\n",
    "            current += char\n",
    "            escape = False\n",
    "        elif char == '\\\\':\n",
    "            escape = True\n",
    "        elif char == '\"':\n",
    "            if in_string:\n",
    "                result.append(current)\n",
    "                current = \"\"\n",
    "            in_string = not in_string\n",
    "        elif in_string:\n",
    "            current += char\n",
    "    return result\n",
    "\n",
    "\n",
    "def zeros(rows, cols=None):\n",
    "    if cols is None:\n",
    "        return [0.0 for _ in range(rows)]\n",
    "    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "\n",
    "def matmul(A, B):\n",
    "    if isinstance(A[0], list) and isinstance(B[0], list):\n",
    "        # Matrix @ Matrix\n",
    "        rows_A, cols_A = len(A), len(A[0])\n",
    "        rows_B, cols_B = len(B), len(B[0])\n",
    "        result = zeros(rows_A, cols_B)\n",
    "        for i in range(rows_A):\n",
    "            for j in range(cols_B):\n",
    "                for k in range(cols_A):\n",
    "                    result[i][j] += A[i][k] * B[k][j]\n",
    "        return result\n",
    "    elif isinstance(A[0], list):\n",
    "        # Matrix @ Vector\n",
    "        result = [0.0 for _ in range(len(A))]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(B)):\n",
    "                result[i] += A[i][j] * B[j]\n",
    "        return result\n",
    "    else:\n",
    "        # Vector @ Matrix\n",
    "        result = [0.0 for _ in range(len(B[0]))]\n",
    "        for j in range(len(B[0])):\n",
    "            for i in range(len(A)):\n",
    "                result[j] += A[i] * B[i][j]\n",
    "        return result\n",
    "\n",
    "\n",
    "def transpose(A):\n",
    "    return [[A[j][i] for j in range(len(A))] for i in range(len(A[0]))]\n",
    "\n",
    "\n",
    "def outer(a, b):\n",
    "    return [[a[i] * b[j] for j in range(len(b))] for i in range(len(a))]\n",
    "\n",
    "\n",
    "def add_matrices(A, B):\n",
    "    if isinstance(A[0], list):\n",
    "        return [[A[i][j] + B[i][j] for j in range(len(A[0]))] for i in range(len(A))]\n",
    "    else:\n",
    "        return [A[i] + B[i] for i in range(len(A))]\n",
    "\n",
    "\n",
    "def sub_matrices(A, B):\n",
    "    if isinstance(A[0], list):\n",
    "        return [[A[i][j] - B[i][j] for j in range(len(A[0]))] for i in range(len(A))]\n",
    "    else:\n",
    "        return [A[i] - B[i] for i in range(len(A))]\n",
    "\n",
    "\n",
    "def mul_matrices(A, B):\n",
    "    if isinstance(A[0], list):\n",
    "        return [[A[i][j] * B[i][j] for j in range(len(A[0]))] for i in range(len(A))]\n",
    "    else:\n",
    "        return [A[i] * B[i] for i in range(len(A))]\n",
    "\n",
    "\n",
    "def div_scalar(A, scalar):\n",
    "    if isinstance(A[0], list):\n",
    "        return [[A[i][j] / scalar for j in range(len(A[0]))] for i in range(len(A))]\n",
    "    else:\n",
    "        return [A[i] / scalar for i in range(len(A))]\n",
    "\n",
    "\n",
    "def mul_scalar(A, scalar):\n",
    "    if isinstance(A[0], list):\n",
    "        return [[A[i][j] * scalar for j in range(len(A[0]))] for i in range(len(A))]\n",
    "    else:\n",
    "        return [A[i] * scalar for i in range(len(A))]\n",
    "\n",
    "\n",
    "def copy_matrix(A):\n",
    "    if isinstance(A[0], list):\n",
    "        return [[A[i][j] for j in range(len(A[0]))] for i in range(len(A))]\n",
    "    else:\n",
    "        return [A[i] for i in range(len(A))]\n",
    "\n",
    "\n",
    "def triu(n, value, k=1):\n",
    "    matrix = zeros(n, n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if j >= i + k:\n",
    "                matrix[i][j] = value\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def argmax(arr):\n",
    "    max_val = arr[0]\n",
    "    max_idx = 0\n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] > max_val:\n",
    "            max_val = arr[i]\n",
    "            max_idx = i\n",
    "    return max_idx\n",
    "\n",
    "\n",
    "def argsort(arr):\n",
    "    return sorted(range(len(arr)), key=lambda i: arr[i])\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    max_z = max(z)\n",
    "    exp_z = [exp(zi - max_z) for zi in z]\n",
    "    sum_exp = sum(exp_z)\n",
    "    return [e / sum_exp for e in exp_z]\n",
    "\n",
    "\n",
    "def save_weights(model_name, vocab, w_embed, w_pos, w_q, w_k, w_v, w_out, b_out, vocab_size, d_model, context_len):\n",
    "    with open('w_embed.txt', 'w') as f:\n",
    "        for row in w_embed:\n",
    "            f.write(','.join([str(x) for x in row]) + '\\n')\n",
    "    with open('w_pos.txt', 'w') as f:\n",
    "        for row in w_pos:\n",
    "            f.write(','.join([str(x) for x in row]) + '\\n')\n",
    "    with open('w_q.txt', 'w') as f:\n",
    "        for row in w_q:\n",
    "            f.write(','.join([str(x) for x in row]) + '\\n')\n",
    "    with open('w_k.txt', 'w') as f:\n",
    "        for row in w_k:\n",
    "            f.write(','.join([str(x) for x in row]) + '\\n')\n",
    "    with open('w_v.txt', 'w') as f:\n",
    "        for row in w_v:\n",
    "            f.write(','.join([str(x) for x in row]) + '\\n')\n",
    "    with open('w_out.txt', 'w') as f:\n",
    "        for row in w_out:\n",
    "            f.write(','.join([str(x) for x in row]) + '\\n')\n",
    "    with open('b_out.txt', 'w') as f:\n",
    "        f.write(','.join([str(x) for x in b_out]) + '\\n')\n",
    "    with open('vocab.txt', 'w') as f:\n",
    "        f.write(','.join(vocab) + '\\n')\n",
    "    with open('w_attn_out.txt', 'w') as f:\n",
    "        f.write(f'{vocab_size}\\n{d_model}\\n{context_len}\\n')\n",
    "    print(f'Trained weights and biases for {model_name} saved to disk!')\n",
    "\n",
    "\n",
    "def load_weights():\n",
    "    with open('w_attn_out.txt', 'r') as f:\n",
    "        vocab_size = int(f.readline().strip())\n",
    "        d_model = int(f.readline().strip())\n",
    "        context_len = int(f.readline().strip())\n",
    "    with open('vocab.txt', 'r') as f:\n",
    "        vocab = f.readline().strip().split(',')\n",
    "    with open('w_embed.txt', 'r') as f:\n",
    "        w_embed = [[float(x) for x in line.strip().split(',')] for line in f.readlines()]\n",
    "    with open('w_pos.txt', 'r') as f:\n",
    "        w_pos = [[float(x) for x in line.strip().split(',')] for line in f.readlines()]\n",
    "    with open('w_q.txt', 'r') as f:\n",
    "        w_q = [[float(x) for x in line.strip().split(',')] for line in f.readlines()]\n",
    "    with open('w_k.txt', 'r') as f:\n",
    "        w_k = [[float(x) for x in line.strip().split(',')] for line in f.readlines()]\n",
    "    with open('w_v.txt', 'r') as f:\n",
    "        w_v = [[float(x) for x in line.strip().split(',')] for line in f.readlines()]\n",
    "    with open('w_out.txt', 'r') as f:\n",
    "        w_out = [[float(x) for x in line.strip().split(',')] for line in f.readlines()]\n",
    "    with open('b_out.txt', 'r') as f:\n",
    "        b_out = [float(x) for x in f.readline().strip().split(',')]\n",
    "    print('Trained weights and biases for SARAN loaded from disk!')\n",
    "    return w_embed, w_pos, w_q, w_k, w_v, w_out, b_out, vocab, vocab_size, d_model, context_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d126ed7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f6819ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['<UNK>', 'a', 'against', 'and', 'as', 'at', 'children', 'day', 'everywhere', 'fleece', 'followed', 'go', 'had', 'her', 'it', 'its', 'lamb', 'laugh', 'little', 'made', 'mary', 'one', 'play', 'rules', 'school', 'see', 'snow', 'sure', 'that', 'the', 'to', 'was', 'went', 'which', 'white']\n",
      "Training samples: 26\n"
     ]
    }
   ],
   "source": [
    "with open('corpus.json') as f:\n",
    "    corpus = parse_json_array(f.read())\n",
    "words = set()\n",
    "for line in corpus:\n",
    "    for word in line.split():\n",
    "        words.add(word)\n",
    "vocab = ['<UNK>'] + sorted(words)\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "context_len = 4\n",
    "data = []\n",
    "for line in corpus:\n",
    "    tokens = line.split()\n",
    "    for i in range(len(tokens) - context_len):\n",
    "        context = [word_to_idx[tokens[j]] for j in range(i, i + context_len)]\n",
    "        target = word_to_idx[tokens[i + context_len]]\n",
    "        data.append((context, target))\n",
    "print(f'Vocabulary: {vocab}')\n",
    "print(f'Training samples: {len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d0e7e9",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2dd49b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 20, Val samples: 6\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(data) * 0.8)  # 80% train, 20% validation\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "print(f'Train samples: {len(train_data)}, Val samples: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b81151",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3c0b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SARAN'\n",
    "n = 0.01  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3cb7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300  # cycles through the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ec863",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0eb26999",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 32\n",
    "\n",
    "w_embed = [[randn() * 0.1 for _ in range(d_model)] for _ in range(vocab_size)]\n",
    "w_pos = [[randn() * 0.1 for _ in range(d_model)] for _ in range(context_len)]\n",
    "\n",
    "w_q = [[randn() * 0.1 for _ in range(d_model)] for _ in range(d_model)]\n",
    "w_k = [[randn() * 0.1 for _ in range(d_model)] for _ in range(d_model)]\n",
    "w_v = [[randn() * 0.1 for _ in range(d_model)] for _ in range(d_model)]\n",
    "\n",
    "w_out = [[randn() * 0.1 for _ in range(vocab_size)] for _ in range(d_model)]\n",
    "b_out = zeros(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c04a7a",
   "metadata": {},
   "source": [
    "## GPT Training w/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f81a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Cost=59.0611, Train Acc=15.00%, Val Cost=20.0826, Val Acc=0.00%\n",
      "Epoch 100: Train Cost=47.2471, Train Acc=15.00%, Val Cost=19.3142, Val Acc=0.00%\n",
      "Epoch 150: Train Cost=32.2077, Train Acc=45.00%, Val Cost=16.1187, Val Acc=16.67%\n",
      "Epoch 200: Train Cost=19.9995, Train Acc=70.00%, Val Cost=14.2715, Val Acc=16.67%\n",
      "Epoch 250: Train Cost=11.2064, Train Acc=95.00%, Val Cost=13.4749, Val Acc=33.33%\n",
      "Epoch 300: Train Cost=4.1649, Train Acc=100.00%, Val Cost=12.2693, Val Acc=66.67%\n"
     ]
    }
   ],
   "source": [
    "sqrt_d_model = sqrt(d_model)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_cost = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    for context, target in train_data:\n",
    "        token_ids = context\n",
    "        \n",
    "        # Forward Pass: x = w_embed[token_ids] + w_pos\n",
    "        x = [[w_embed[tid][j] + w_pos[i][j] for j in range(d_model)] for i, tid in enumerate(token_ids)]\n",
    "        \n",
    "        # q = x @ w_q, k = x @ w_k, v = x @ w_v\n",
    "        q = matmul(x, w_q)\n",
    "        k = matmul(x, w_k)\n",
    "        v = matmul(x, w_v)\n",
    "        \n",
    "        # scores = q @ k.T / sqrt(d_model)\n",
    "        k_T = transpose(k)\n",
    "        scores = matmul(q, k_T)\n",
    "        scores = div_scalar(scores, sqrt_d_model)\n",
    "        \n",
    "        # mask and add to scores\n",
    "        mask = triu(context_len, -1e9, k=1)\n",
    "        scores = add_matrices(scores, mask)\n",
    "        \n",
    "        # attn = softmax(scores)\n",
    "        attn = [softmax(row) for row in scores]\n",
    "        \n",
    "        # attn_out = attn @ v\n",
    "        attn_out = matmul(attn, v)\n",
    "        \n",
    "        # last = attn_out[-1]\n",
    "        last = attn_out[-1]\n",
    "        \n",
    "        # logits = last @ w_out + b_out\n",
    "        logits = matmul(last, w_out)\n",
    "        logits = add_matrices(logits, b_out)\n",
    "        \n",
    "        # o = softmax(logits)\n",
    "        o = softmax(logits)\n",
    "        \n",
    "        # Compute Cost\n",
    "        c = -log(o[target] + 1e-8)\n",
    "        total_cost += c\n",
    "        \n",
    "        # Compute Accuracy\n",
    "        if argmax(o) == target:\n",
    "            correct += 1\n",
    "        \n",
    "        # Backpropagation\n",
    "        do = copy_matrix(o)\n",
    "        do[target] -= 1\n",
    "        \n",
    "        # dw_out = outer(last, do)\n",
    "        dw_out = outer(last, do)\n",
    "        db_out = copy_matrix(do)\n",
    "        \n",
    "        # dlast = do @ w_out.T\n",
    "        w_out_T = transpose(w_out)\n",
    "        dlast = matmul(do, w_out_T)\n",
    "        \n",
    "        # dattn_out = zeros_like(attn_out)\n",
    "        dattn_out = zeros(context_len, d_model)\n",
    "        dattn_out[-1] = dlast\n",
    "        \n",
    "        # dv = attn.T @ dattn_out\n",
    "        attn_T = transpose(attn)\n",
    "        dv = matmul(attn_T, dattn_out)\n",
    "        \n",
    "        # dw_v = x.T @ dv\n",
    "        x_T = transpose(x)\n",
    "        dw_v = matmul(x_T, dv)\n",
    "        \n",
    "        # dattn = dattn_out @ v.T\n",
    "        v_T = transpose(v)\n",
    "        dattn = matmul(dattn_out, v_T)\n",
    "        \n",
    "        # dscores = attn * (dattn - sum(dattn * attn, axis=1, keepdims=True))\n",
    "        attn_dattn = mul_matrices(dattn, attn)\n",
    "        sum_attn_dattn = [sum(row) for row in attn_dattn]\n",
    "        dattn_adjusted = [[dattn[i][j] - sum_attn_dattn[i] for j in range(len(dattn[0]))] for i in range(len(dattn))]\n",
    "        dscores = mul_matrices(attn, dattn_adjusted)\n",
    "        dscores = div_scalar(dscores, sqrt_d_model)\n",
    "        \n",
    "        # dq = dscores @ k\n",
    "        dq = matmul(dscores, k)\n",
    "        \n",
    "        # dk = dscores.T @ q\n",
    "        dscores_T = transpose(dscores)\n",
    "        dk = matmul(dscores_T, q)\n",
    "        \n",
    "        # dw_q = x.T @ dq\n",
    "        dw_q = matmul(x_T, dq)\n",
    "        \n",
    "        # dw_k = x.T @ dk\n",
    "        dw_k = matmul(x_T, dk)\n",
    "        \n",
    "        # dx = dq @ w_q.T + dk @ w_k.T + dv @ w_v.T\n",
    "        w_q_T = transpose(w_q)\n",
    "        w_k_T = transpose(w_k)\n",
    "        w_v_T = transpose(w_v)\n",
    "        dx1 = matmul(dq, w_q_T)\n",
    "        dx2 = matmul(dk, w_k_T)\n",
    "        dx3 = matmul(dv, w_v_T)\n",
    "        dx = add_matrices(add_matrices(dx1, dx2), dx3)\n",
    "        \n",
    "        # dw_embed = zeros_like(w_embed)\n",
    "        dw_embed = zeros(vocab_size, d_model)\n",
    "        for i, tid in enumerate(token_ids):\n",
    "            for j in range(d_model):\n",
    "                dw_embed[tid][j] += dx[i][j]\n",
    "        \n",
    "        # dw_pos = dx.copy()\n",
    "        dw_pos = copy_matrix(dx)\n",
    "        \n",
    "        # Update Parameters\n",
    "        w_out = sub_matrices(w_out, mul_scalar(dw_out, n))\n",
    "        b_out = sub_matrices(b_out, mul_scalar(db_out, n))\n",
    "        w_v = sub_matrices(w_v, mul_scalar(dw_v, n))\n",
    "        w_q = sub_matrices(w_q, mul_scalar(dw_q, n))\n",
    "        w_k = sub_matrices(w_k, mul_scalar(dw_k, n))\n",
    "        w_embed = sub_matrices(w_embed, mul_scalar(dw_embed, n))\n",
    "        w_pos = sub_matrices(w_pos, mul_scalar(dw_pos, n))\n",
    "    \n",
    "    train_accuracy = correct / len(train_data) * 100\n",
    "    \n",
    "    # Validation Loop (No Backpropagation)\n",
    "    val_correct = 0\n",
    "    val_cost = 0\n",
    "    for context, target in val_data:\n",
    "        token_ids = context\n",
    "        \n",
    "        # Forward Pass\n",
    "        x = [[w_embed[tid][j] + w_pos[i][j] for j in range(d_model)] for i, tid in enumerate(token_ids)]\n",
    "        q = matmul(x, w_q)\n",
    "        k = matmul(x, w_k)\n",
    "        v = matmul(x, w_v)\n",
    "        k_T = transpose(k)\n",
    "        scores = matmul(q, k_T)\n",
    "        scores = div_scalar(scores, sqrt_d_model)\n",
    "        mask = triu(context_len, -1e9, k=1)\n",
    "        scores = add_matrices(scores, mask)\n",
    "        attn = [softmax(row) for row in scores]\n",
    "        attn_out = matmul(attn, v)\n",
    "        last = attn_out[-1]\n",
    "        logits = matmul(last, w_out)\n",
    "        logits = add_matrices(logits, b_out)\n",
    "        o = softmax(logits)\n",
    "        \n",
    "        # Compute Validation Cost\n",
    "        c = -log(o[target] + 1e-8)\n",
    "        val_cost += c\n",
    "        \n",
    "        # Compute Validation Accuracy\n",
    "        if argmax(o) == target:\n",
    "            val_correct += 1\n",
    "    \n",
    "    val_accuracy = val_correct / len(val_data) * 100\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch {epoch+1}: Train Cost={total_cost:.4f}, Train Acc={train_accuracy:.2f}%, Val Cost={val_cost:.4f}, Val Acc={val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ca2da",
   "metadata": {},
   "source": [
    "## Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38ca9d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights and biases for SARAN saved to disk!\n"
     ]
    }
   ],
   "source": [
    "save_weights(model_name, vocab, w_embed, w_pos, w_q, w_k, w_v, w_out, b_out, vocab_size, d_model, context_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac73960",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81ddb95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights and biases for SARAN loaded from disk!\n",
      "Loaded vocabulary with 35 words\n",
      "Model parameters: vocab_size=35, d_model=32, context_len=4\n"
     ]
    }
   ],
   "source": [
    "w_embed, w_pos, w_q, w_k, w_v, w_out, b_out, vocab, vocab_size, d_model, context_len = load_weights()\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "print(f'Loaded vocabulary with {len(vocab)} words')\n",
    "print(f'Model parameters: vocab_size={vocab_size}, d_model={d_model}, context_len={context_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1cee19",
   "metadata": {},
   "source": [
    "## Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe294766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [20, 12, 1, 18]\n",
      "Tokens mapped: ['mary', 'had', 'a', 'little']\n",
      "\n",
      "Input: mary had a little\n",
      "Predicted: lamb\n",
      "\n",
      "Top 5 predictions:\n",
      "  lamb: 0.9338\n",
      "  went: 0.0612\n",
      "  as: 0.0022\n",
      "  school: 0.0012\n",
      "  laugh: 0.0007\n",
      "\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_input = ['mary', 'had', 'a', 'little']\n",
    "\n",
    "token_ids = tokens_to_ids(test_input, word_to_idx)\n",
    "\n",
    "print(f'Token IDs: {token_ids}')\n",
    "print(f'Tokens mapped: {[idx_to_word[tid] for tid in token_ids]}')\n",
    "\n",
    "# Forward Pass\n",
    "sqrt_d_model = sqrt(d_model)\n",
    "x = [[w_embed[tid][j] + w_pos[i][j] for j in range(d_model)] for i, tid in enumerate(token_ids)]\n",
    "q = matmul(x, w_q)\n",
    "k = matmul(x, w_k)\n",
    "v = matmul(x, w_v)\n",
    "k_T = transpose(k)\n",
    "scores = matmul(q, k_T)\n",
    "scores = div_scalar(scores, sqrt_d_model)\n",
    "mask = triu(context_len, -1e9, k=1)\n",
    "scores = add_matrices(scores, mask)\n",
    "attn = [softmax(row) for row in scores]\n",
    "attn_out = matmul(attn, v)\n",
    "last = attn_out[-1]\n",
    "logits = matmul(last, w_out)\n",
    "logits = add_matrices(logits, b_out)\n",
    "o = softmax(logits)\n",
    "\n",
    "print(f'\\nInput: {\" \".join(test_input)}')\n",
    "print(f'Predicted: {idx_to_word[argmax(o)]}')\n",
    "print(f'\\nTop 5 predictions:')\n",
    "top5_idx = argsort(o)[-5:][::-1]\n",
    "for idx in top5_idx:\n",
    "    print(f'  {idx_to_word[idx]}: {o[idx]:.4f}')\n",
    "print(f'\\nSum: {sum(o)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
